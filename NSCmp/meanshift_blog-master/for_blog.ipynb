{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_vectors=20000\n",
    "vector_dimension=2\n",
    "data=np.random.uniform(-1, 1, size=(n_vectors, vector_dimension))\n",
    "unif=np.random.uniform(size=n_vectors)\n",
    "scale_f=np.expand_dims(np.linalg.norm(data, axis=1)/unif, axis=1)\n",
    "data=data/scale_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gaussian(d, bw):\n",
    "    return np.exp(-0.5*(d/bw)**2) / (bw*np.sqrt(2*np.pi))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#looped version\n",
    "def _dist_poinc(a, b):\n",
    "    num=np.dot(a-b, a-b)\n",
    "    den1=1-np.dot(a,a)\n",
    "    den2=1-np.dot(b,b)\n",
    "    return np.arccosh(1+ 2* (num) / (den1*den2))\n",
    "def dist_poinc(a, A):\n",
    "    res=np.empty(A.shape[0])\n",
    "    for i, el in enumerate(A):\n",
    "        res[i]=_dist_poinc(a, el)\n",
    "    return res\n",
    "\n",
    "def meanshift(data, sigma, steps):\n",
    "    d1 = np.copy(data)                        # Need to copy the data, don't want to modify the originals\n",
    "    for it in range(steps):                   # at each step\n",
    "        for i, p in enumerate(d1):            # for each point\n",
    "            dists = dist_poinc( p, d1)        # we calculate the distance from that point to all the other ones\n",
    "            weights = gaussian(dists, sigma)  # then we weight those distances by our gaussian kernel\n",
    "            d1[i] = (np.expand_dims(weights,1)*d1).sum(0) / weights.sum()     # and substitute the point with the weighted sum\n",
    "    return d1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vectorized version\n",
    "def num(points):\n",
    "    expd=np.expand_dims(points,2) #need another dimension...\n",
    "    tiled=np.tile(expd, points.shape[0]) #...to tile up the vectors\n",
    "    trans=np.transpose(points) #Also need to transpose the points matrix to fit well with broadcasting\n",
    "    diff=trans-tiled           #doing the difference, exploiting Numpy broadcasting capabilities\n",
    "    num=np.sum(np.square(diff), axis=1) #an then obtain the squared norm of the difference\n",
    "    return num\n",
    "\n",
    "def den(points):\n",
    "    sq_norm=1-np.sum(np.square(points),1) #subtracting from 1 the squared norm of the vectors\n",
    "    expd=np.expand_dims(sq_norm,1)   #this operation is needed to obtain a correctly transposed version of the vector\n",
    "    den_all=expd * expd.T #multiply the object by his transpose\n",
    "    return den_all\n",
    "\n",
    "def poinc_dist_vec(points):\n",
    "    numa=num(points)\n",
    "    dena=den(points)\n",
    "    return np.arccosh(1+2*numa/ dena)\n",
    "\n",
    "def meanshift_vec(points, sigma):\n",
    "    dists=poinc_dist_vec(points) #the matrix of the distances\n",
    "    weights = gaussian(dists, sigma) #the matrix of the weights\n",
    "    expd_w=np.dot(weights, points) #the weighted vectors\n",
    "    summed_weight=np.sum(weights,0) # the array of the summed weights, for normalize the weighted vectors\n",
    "    shifted_pts=expd_w/np.expand_dims(summed_weight,1) #the normalized vectors\n",
    "    return shifted_pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parallel version\n",
    "import concurrent\n",
    "\n",
    "def dist_batch(points,  n_begin, n_end):\n",
    "    expd=np.expand_dims(points,2)\n",
    "    tiled=np.tile(expd, n_end-n_begin)\n",
    "    selected=points[n_begin:n_end]\n",
    "    trans=np.transpose(selected)\n",
    "    num=np.sum(np.square(trans-tiled), axis=1)\n",
    "\n",
    "    den_sq_norm=1-np.sum(np.square(points),1)\n",
    "    den_selected=den_sq_norm[n_begin:n_end]\n",
    "    den_expd=np.expand_dims(den_sq_norm,1)\n",
    "    den=den_expd * den_selected.T\n",
    "\n",
    "    return np.arccosh(1+2*num/ den)\n",
    "\n",
    "def __shift(data_all, sigma, beg, end):\n",
    "    dists=dist_batch(data_all, beg, end)\n",
    "    weights = gaussian(dists, sigma)\n",
    "    expd_w=np.dot(weights.T, data_all)\n",
    "    summed_weight=np.sum(weights,0)\n",
    "    return expd_w/np.expand_dims(summed_weight,1), beg, end\n",
    "\n",
    "\n",
    "def meanshift_parallel(data,sigma,  batches):\n",
    "    pts = np.copy(data)\n",
    "    n_samples=data.shape[0]\n",
    "    processes_needed=int(np.ceil(n_samples/batches))\n",
    "    maxworkers=min (6, processes_needed)\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=maxworkers) as executor:\n",
    "        future_shifted = {executor.submit(__shift, pts, sigma, d_beg, min(d_beg+batches, n_samples)):\n",
    "                         d_beg for d_beg in range(0, n_samples, batches)}\n",
    "        for future in concurrent.futures.as_completed(future_shifted):\n",
    "            pts[future.result()[1]:future.result()[2]]=future.result()[0]\n",
    "\n",
    "\n",
    "    return pts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#For plotting the data\n",
    "import matplotlib.pyplot as plt\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = plt.gca()\n",
    "ax.cla() # clear things for fresh plot\n",
    "\n",
    "ax.set_xlim((-1.1, 1.1))\n",
    "ax.set_ylim((-1.1, 1.1))\n",
    "circle = plt.Circle((0,0), 1., color='black', fill=False)\n",
    "ax.add_artist(circle)\n",
    "plt.axis('off')\n",
    "for point in data:\n",
    "    ax.plot(point[0], point[1], 'o', color='b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#to calculate the execution times\n",
    "import timeit\n",
    "times_looped=[]\n",
    "times_parallel=[]\n",
    "\n",
    "\n",
    "dim=[1000, 2000, 4000, 6000, 8000, 10000]\n",
    "for i in dim:\n",
    "    print (i)\n",
    "    steps=2\n",
    "    temp_times_looped=np.zeros(steps)\n",
    "    temp_times_parallel=np.zeros(steps)\n",
    "    for j in range(steps):\n",
    "        print (\"--\", j)\n",
    "        # looped\n",
    "        start_time = timeit.default_timer()\n",
    "        meanshift(data[:i], sigma=0.8, steps=1)\n",
    "        temp_times_looped[j]=timeit.default_timer() - start_time\n",
    "        \n",
    "        # parallel, batch 100\n",
    "        start_time = timeit.default_timer()\n",
    "        meanshift_parallel(data[:i], sigma=0.8, batches=100)\n",
    "        temp_times_parallel[j]=timeit.default_timer() - start_time\n",
    "        \n",
    "        \n",
    "    temp_time_looped=np.sum(temp_times_looped)/temp_times_looped.shape[0]\n",
    "    temp_time_parallel=np.sum(temp_times_parallel)/temp_times_parallel.shape[0]\n",
    "    times_looped.append( temp_time_looped)\n",
    "    times_parallel.append( temp_time_parallel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot the execution times\n",
    "from matplotlib import pyplot as plt\n",
    "plt.plot(dim,times_parallel,label='batched & parallel, batch 100')\n",
    "\n",
    "plt.plot(dim,times_looped,label='looped')\n",
    "\n",
    "\n",
    "plt.legend()\n",
    "plt.ylabel('Execution time')\n",
    "plt.xlabel('number of vectors')\n",
    "plt.yscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.ticker as mtick\n",
    "\n",
    "percs=np.array(times_parallel)/np.array(times_looped)\n",
    "\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(1,1,1)\n",
    "\n",
    "ax.plot(dim,percs*100,label='batched & parallel / looped')\n",
    "\n",
    "ax.legend()\n",
    "ax.set_ylabel('Execution time')\n",
    "ax.set_xlabel('number of vectors')\n",
    "fmt = '%.2f%%'\n",
    "xticks = mtick.FormatStrFormatter(fmt)\n",
    "ax.yaxis.set_major_formatter(xticks)\n",
    "#plt.yscale('log')\n",
    "#ax.set_ylim(0.20000, 0.630000)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
